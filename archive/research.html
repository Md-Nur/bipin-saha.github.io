<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bipin's Research</title>
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            background-color: #f0f2f5;
            color: #333;
            margin: 0;
            padding: 0;
        }
        header {
            background-color: #007acc;
            color: #fff;
            padding: 20px 0;
            text-align: center;
        }
        .container {
            width: 90%;
            max-width: 1200px;
            margin: 20px auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        .paper {
            margin-bottom: 30px;
            padding: 20px;
            border-bottom: 1px solid #ddd;
        }
        .paper h2 {
            margin-top: 0;
            color: #007acc;
        }
        .paper p {
            line-height: 1.6;
        }
        .featured-paper {
            background-color: #e3f2fd;
            border-left: 5px solid #007acc;
        }
        .featured-paper h2 {
            color: #005f99;
        }
        .insights {
            margin-top: 15px;
            padding: 15px;
            background-color: #e3f2fd;
            border-left: 5px solid #007acc;
        }
        .insights h3 {
            margin-top: 0;
            color: #007acc;
        }
        .toggle-button {
            background-color: #007acc;
            color: #fff;
            border: none;
            padding: 10px 15px;
            cursor: pointer;
            border-radius: 5px;
            margin-top: 10px;
            transition: background-color 0.3s;
        }
        .toggle-button:hover {
            background-color: #005f99;
        }
        .thumbnail-row {
            display: flex;
            justify-content: center;
            gap: 10px;
            margin-top: 10px;
        }
        .thumbnail {
            cursor: pointer;
            border: 2px solid transparent;
            transition: border-color 0.3s;
            width: 100px;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        .thumbnail:hover {
            border-color: #007acc;
        }
        .thumbnail.active {
            border-color: #007acc;
        }
        /* Center main images */
        .main-image {
            display: block;
            margin: 0 auto;
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        /* Modal styles */
        .modal {
            display: none;
            position: fixed;
            z-index: 1;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            overflow: auto;
            background-color: rgb(0,0,0);
            background-color: rgba(0,0,0,0.9);
        }
        .modal-content {
            margin: 15% auto;
            display: block;
            width: 80%;
            max-width: 700px;
        }
        .close {
            position: absolute;
            top: 15px;
            right: 35px;
            color: #fff;
            font-size: 40px;
            font-weight: bold;
            transition: 0.3s;
            cursor: pointer;
        }
        .close:hover,
        .close:focus {
            color: #bbb;
            text-decoration: none;
            cursor: pointer;
        }
        .modal-content, .thumbnail {
            border-radius: 10px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Research</h1>
    </header>
    <div class="container">
        <div class="paper featured-paper">
            <h2>BNVD: Bangladeshi Native Vehicle Detection in Wild</h2>
            <img id="main-image-1" class="main-image" src="research_images/bnvd_result.jpg" alt="BNVD Detection with YOLO v8">
            <div class="thumbnail-row">
                <img class="thumbnail" src="research_images/bnvd_ds_overview.jpg" alt="Thumbnail 1" onclick="openModal('research_images/bnvd_ds_overview.jpg')">
                <img class="thumbnail" src="research_images/bnvd_Rickshaw_Types.jpg" alt="Thumbnail 2" onclick="openModal('research_images/bnvd_Rickshaw_Types.jpg')">
                <img class="thumbnail" src="research_images/bnvd_Lighting_Condition.jpg" alt="Thumbnail 3" onclick="openModal('research_images/bnvd_Lighting_Condition.jpg')">
            </div>
            <p>Abstract: The success of autonomous navigation relies on robust and precise vehicle recognition, hindered by the scarcity of region-specific vehicle detection datasets, impeding the development of context-aware systems. To advance terrestrial object detection research, this paper proposes a native vehicle detection dataset for the most commonly appeared vehicle classes in Bangladesh. 17 distinct vehicle classes have been taken into account, with fully annotated 81542 instances of 17326 images. Each image width is set to at least 1280px. The dataset’s average vehicle bounding box-to-image ratio is 4.7036. This Bangladesh Native Vehicle Dataset (BNVD) has accounted for several geographical, illumination, variety of vehicle sizes, and orientations to be more robust on surprised scenarios. In the context of examining the BNVD dataset, this work provides a thorough assessment with four successive You Only Look Once (YOLO) models, namely YOLO v5, v6, v7, and v8. These dataset’s effectiveness is methodically evaluated and contrasted with other vehicle datasets already in use. The BNVD dataset exhibits mean average precision(mAP) at 50% intersection over union(IoU) is 0.848 corresponding precision and recall values of 0.841 and 0.774. The research findings indicate a mAP of 0.643 at an IoU range of 0.5 to 0.95. The experiments show that the BNVD dataset serves as a reliable representation of vehicle distribution and presents considerable complexities.</p>
            <button class="toggle-button" onclick="toggleInsights(1)">Show Insights</button>
            <div class="insights" id="insights-1" style="display: none;">
                <h3>Key Contributions</h3>
                <p>So far, the proposed BNVD is the largest establishment of
                    datasets of View-Point Invariant Vehicle Detection in the Bangladeshi environment, with a wide variety of classes,
                    highly focused on native classes </p>
                    <p>The research delved into identifying the most suitable
                    model for vehicle detection, exploring performance fluctuations resulting from the sequential transition between
                    different models. </p>
                    <p>This study investigated how category-specific performance impacts the overall effectiveness of the detectors
                    employed in this analysis.</p>
                    <p>Furthermore, an assessment of performance was conducted on subsets characterized by adverse weather conditions and varying light conditions. This evaluation
                    utilized the two most effective models identified in the
                    study.</p>
            </div>
        </div>

        <div class="paper">
            <h2>A Transformer-Based Approach for Summarizing Employee Logs</h2>
            <img id="main-image-2" class="main-image" src="research_images/textsum_overview.jpeg" alt="Research Paper 2 Image">
            <div class="thumbnail-row">
                <img class="thumbnail" src="research_images/textsum_overview.jpeg" alt="Thumbnail 1" onclick="openModal('research_images/textsum_overview.jpeg')">
                <img class="thumbnail" src="research_images/textsum_rouge.png" alt="Thumbnail 2" onclick="openModal('research_images/textsum_rouge.png')">
                <img class="thumbnail" src="research_images/textsum_loss.png" alt="Thumbnail 3" onclick="openModal('research_images/textsum_loss.png')">
            </div>
            <p>Abstract: Efficient summarization of employee daily logs is crucial for organizational productivity. This study utilizes the Google Pegasus model to enhance summarization, addressing traditional challenges with complex document structures. A dataset of daily work logs from a software development firm was augmented with advanced Generative AI techniques. The Pegasus model was fine-tuned with an input token length of 1024 and an output token length of 128. Evaluated using the ROUGE metric, the model achieved a ROUGE-1 score of 0.613, ROUGE-2 score of 0.373, ROUGE-L score of 0.557, and ROUGE-Lsum score of 0.556, demonstrating strong performance in generating coherent and contextually relevant summaries.</p>
            <button class="toggle-button" onclick="toggleInsights(2)">Show Insights</button>
            <div class="insights" id="insights-2" style="display: none;">
                <h3>Insights</h3>
                <p>Insight 1: </p>
                <p>Insight 2: </p>
            </div>
        </div>
        <!-- Add more papers as needed -->
        
        <div class="paper">
            <h2>An Efficient Approach for Appearance Based Eye Gaze Estimation with 13 Directional Points</h2>
            <img id="main-image-2" class="main-image" src="https://via.placeholder.com/400x200" alt="Research Paper 2 Image">
            <div class="thumbnail-row">
                <img class="thumbnail" src="https://via.placeholder.com/400x200" alt="Thumbnail 1" onclick="openModal('https://via.placeholder.com/400x200')">
                <img class="thumbnail" src="https://via.placeholder.com/400x200/007acc" alt="Thumbnail 2" onclick="openModal('https://via.placeholder.com/400x200/007acc')">
                <img class="thumbnail" src="https://via.placeholder.com/400x200/ff0000" alt="Thumbnail 3" onclick="openModal('https://via.placeholder.com/400x200/ff0000')">
            </div>
            <p>Abstract: This paper proposes a real-time eye gaze tracking interface based on an active appearance method using a simple web or smartphone camera in an unconstrained environment, where natural head movements have been taken into account. Here, feature extraction from an eye gaze image has been performed by separating the sclera pixel area from the masked eye image. The performance evaluation has been accomplished using decision tree, random forest, and extra tree classifiers. Over 13 targets, the proposed approach has an approximate 98% accuracy with extra tree classifier. The proposed approach may be beneficial to different eye gazed tracking applications in the field of human-computer interaction, robotics, and medical science.</p>
            <button class="toggle-button" onclick="toggleInsights(2)">Show Insights</button>
            <div class="insights" id="insights-2" style="display: none;">
                <h3>Insights</h3>
                <p>Insight 1: Vivamus lacinia odio vitae vestibulum vestibulum.</p>
                <p>Insight 2: Cras venenatis euismod malesuada.</p>
            </div>
        </div>
        
        <div class="paper">
            <h2>Numerical Modeling of CuSbSe2-based Dual-Heterojunction Thin Film Solar Cell with CGS Back Surface Layer</h2>
            <img id="main-image-2" class="main-image" src="https://via.placeholder.com/400x200" alt="Research Paper 2 Image">
            <div class="thumbnail-row">
                <img class="thumbnail" src="https://via.placeholder.com/400x200" alt="Thumbnail 1" onclick="openModal('https://via.placeholder.com/400x200')">
                <img class="thumbnail" src="https://via.placeholder.com/400x200/007acc" alt="Thumbnail 2" onclick="openModal('https://via.placeholder.com/400x200/007acc')">
                <img class="thumbnail" src="https://via.placeholder.com/400x200/ff0000" alt="Thumbnail 3" onclick="openModal('https://via.placeholder.com/400x200/ff0000')">
            </div>
            <p>Abstract: Ternary chalcostibite copper antimony selenide (CuSbSe2) can be a potential absorber for succeeding thin film solar cells due to its non-toxic nature, earth-abundance, low-cost fabrication technique, optimum bandgap, and high optical absorption coefficient. The power conversion efficiencies (PCEs) in conventional single heterojunction CuSbSe2 solar cells suffer from higher recombination rate at the interfaces and the presence of a Schottky barrier at the back contact. In this study, we propose a dual-heterojunction n-ZnSe/p-CuSbSe2/p+-copper gallium selenide (CGS) solar device, having CGS as the back surface field (BSF) layer. The BSF layer absorbs low energy (sub-bandgap) light through a tail-states-assisted upconversion technique, leading to enhanced conversion efficiency. Numerical simulations were run in Solar Cell Capacitance Simulator-1 dimensional software to examine how the performance of the proposed solar cell would respond under different conditions of absorber layer thickness, doping levels, and defect densities. The simulation results exhibit a PCE as high as 43.77% for the dual-heterojunction solar cell as compared to 27.74% for the single heterojunction n-ZnSe/p-CuSbSe2 counterpart, demonstrating the capability of approaching the detailed balance efficiency limit calculated by Shockley–Queisser.</p>
            <button class="toggle-button" onclick="toggleInsights(2)">Show Insights</button>
            <div class="insights" id="insights-2" style="display: none;">
                <h3>Insights</h3>
                <p>Insight 1: Vivamus lacinia odio vitae vestibulum vestibulum.</p>
                <p>Insight 2: Cras venenatis euismod malesuada.</p>
            </div>
        </div>
    </div>

    <!-- Modal -->
    <div id="imageModal" class="modal">
        <span class="close" onclick="closeModal()">&times;</span>
        <img class="modal-content" id="modal-image">
    </div>

    <script>
        function toggleInsights(id) {
            var insights = document.getElementById('insights-' + id);
            if (insights.style.display === 'none') {
                insights.style.display = 'block';
            } else {
                insights.style.display = 'none';
            }
        }

        function openModal(imageUrl) {
            var modal = document.getElementById('imageModal');
            var modalImg = document.getElementById('modal-image');
            modal.style.display = "block";
            modalImg.src = imageUrl;
        }

        function closeModal() {
            var modal = document.getElementById('imageModal');
            modal.style.display = "none";
        }

        window.onclick = function(event) {
            var modal = document.getElementById('imageModal');
            if (event.target == modal) {
                modal.style.display = "none";
            }
        }
    </script>
</body>
</html>
